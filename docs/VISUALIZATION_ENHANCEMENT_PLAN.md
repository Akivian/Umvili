# 智能体训练可视化增强方案

## 一、现状分析

### 当前可视化能力（最新）
- ✅ 环境可视化：糖分布地图、智能体位置
- ✅ 整体统计：种群数量、平均糖量、多样性指数
- ✅ 智能体训练指标：
  - 训练损失（按算法类型：IQL / QMIX 等）
  - Q值变化趋势
  - TD 误差与探索率
- ✅ Agent 类型 & 分布：数量、占比、平均糖量，区分 RL 算法与规则类智能体
- ❌ **仍待实现**：智能体行为策略可视化（动作分布、奖励分布、策略熵等）

### 数据来源分析

#### IQL智能体 (`src/marl/iql_agent.py`)
- `training_stats` 包含：
  - `q_values`: Q值历史
  - `losses`: 损失函数历史
  - `td_errors`: TD误差历史
  - `exploration_rate`: 探索率历史
  - `rewards`: 奖励历史
- `get_training_info()`: 返回训练信息摘要

#### QMIX训练器 (`src/marl/qmix_trainer.py`)
- `training_stats` 包含：
  - `losses`: 联合损失
  - `q_values`: 联合Q值
  - `td_errors`: TD误差
  - `mixing_losses`: 混合网络损失
- `get_training_stats()`: 返回详细训练统计

#### QMIX智能体 (`src/marl/qmix_agent.py`)
- `training_stats` 包含：
  - `q_values`: Q值历史
  - `losses`: 损失历史
  - `td_errors`: TD误差
  - `exploration_rate`: 探索率
- `get_training_info()`: 返回训练信息

## 二、可视化方案设计

### 2.1 核心指标可视化

#### 阶段1：基础训练指标（优先级：高）
1. **损失函数曲线**
   - 显示所有学习型智能体的平均损失
   - 支持按智能体类型分组显示（IQL、QMIX）
   - 实时更新，显示最近N步的损失趋势

2. **Q值变化趋势**
   - 显示平均Q值随时间的变化
   - 支持按智能体类型分组
   - 显示Q值的滑动平均，减少噪声

#### 阶段2：高级训练指标（优先级：中）
3. **TD误差曲线**
   - 显示时序差分误差的变化
   - 帮助判断学习稳定性

4. **探索率曲线**
   - 显示ε-贪婪策略的探索率衰减
   - 帮助理解探索-利用平衡

#### 阶段3：行为策略可视化（优先级：中-低）
5. **动作分布热图**
   - 显示智能体在不同状态下的动作选择分布
   - 帮助理解策略学习过程

6. **奖励分布**
   - 显示奖励的分布和趋势
   - 帮助评估奖励设计是否合理

### 2.2 技术实现方案（已完成部分 & 待实现部分）

#### 数据收集层（✅ 已完成）
- 在 `simulation.py` 的 `get_simulation_data()` 中添加 `training_metrics` 字段
- 从智能体和训练器收集训练统计信息，并按智能体类型聚合

#### 可视化层（✅ 已完成：训练指标 & 类型分布）
- 使用 `MultiLineChart` 统一实现训练曲线（Loss / Q / TD / Exploration）
- `AgentDistributionPanel` 展示类型数量、占比和平均糖量，并标注 RL 算法类型

#### 性能优化（进行中）
- 已使用滑动窗口（最近 ~200 点）与更新频率（如每5/10步）控制训练图表更新
- 后续可针对 Behavior 视图进一步降低更新频率（如每10或20步）

## 三、逐级实现路线

### 阶段1：数据收集基础设施（✅ 已完成）

#### 1.1 扩展 SimulationMetrics
**文件**: `src/core/simulation.py`

```python
@dataclass
class TrainingMetrics:
    """训练指标数据类"""
    agent_type: str
    avg_loss: float = 0.0
    avg_q_value: float = 0.0
    avg_td_error: float = 0.0
    exploration_rate: float = 0.0
    training_steps: int = 0
    sample_count: int = 0
```

#### 1.2 在 Simulation 中添加训练数据收集
**文件**: `src/core/simulation.py`

- 在 `_collect_metrics()` 中添加 `_collect_training_metrics()` 调用
- 实现 `_collect_training_metrics()` 方法：
  - 遍历所有智能体，收集训练信息
  - 从训练器收集统计信息
  - 按类型聚合数据

#### 1.3 在 get_simulation_data() 中暴露训练数据
- 添加 `training_metrics` 字段到返回字典

**验收标准**:
- [ ] 能够从IQL智能体收集训练统计
- [ ] 能够从QMIX训练器收集训练统计
- [ ] 数据按智能体类型正确聚合

---

### 阶段2：基础图表实现（✅ 已完成）

#### 2.1 扩展 RealTimeChart 支持多线显示
**文件**: `src/utils/visualization.py`

- 添加 `MultiLineChart` 类，支持多条曲线
- 每条曲线可以有不同的颜色和标签
- 支持动态添加/移除曲线

#### 2.2 实现损失函数图表
**文件**: `src/utils/visualization.py`

- 在 `AcademicVisualizationSystem._initialize_charts()` 中添加损失图表
- 在 `_update_charts()` 中更新损失数据
- 支持按类型显示（IQL、QMIX分别显示）

#### 2.3 实现Q值趋势图表
**文件**: `src/utils/visualization.py`

- 添加Q值图表组件
- 显示平均Q值随时间变化
- 支持多类型智能体对比

**验收标准**:
- [ ] 损失函数曲线能够实时显示
- [ ] Q值趋势曲线能够实时显示
- [ ] 图表布局合理，不影响现有功能
- [ ] 性能良好，不影响模拟速度

---

### 阶段3：高级指标可视化（✅ 已完成：TD误差 / 探索率，图表已接入）

#### 3.1 TD误差曲线
- 添加TD误差图表
- 显示误差的变化趋势和稳定性

#### 3.2 探索率曲线
- 添加探索率图表
- 显示ε-贪婪策略的衰减过程

**验收标准**:
- [ ] 所有新增图表都能正常显示
- [ ] 数据更新及时准确

---

### 阶段4：布局与交互基础重构（进行中 / 下一阶段重点之一）

#### 4.1 视图系统（Overview / Training / Behavior / Debug）
- 在 `AcademicVisualizationSystem` 中引入 `active_view` 状态与 Tab 切换：
  - Overview：环境统计 + Agent 类型分布
  - Training：四张训练曲线（Loss / Q / TD / Exploration）
  - Behavior：行为/策略可视化（动作分布、奖励趋势、策略熵）
  - Debug：性能与调试信息（FPS、step time、buffer 利用率等）

#### 4.2 图表布局抽象
- 实现一个小型布局工具，用于根据行列数生成图表槽位 `rect`：
  - Training 视图采用 2×2 布局，保证每张图有足够高度。
  - Behavior 视图复用同一套布局逻辑。

#### 4.3 性能与错误处理
- 统一控制各视图中图表的更新频率与数据窗口长度。
- 在行为可视化中加强异常值过滤与数据缺失保护。

**验收标准**:
- [ ] 视图切换逻辑清晰，UI 不再拥挤
- [ ] 训练与行为图表在各自视图中拥有良好可读性
- [ ] FPS ≥ 30，长时间运行稳定

---

### 阶段5：行为策略可视化（下一阶段核心目标，预计：4–6 小时）

#### 5.1 动作分布可视化
- 数据收集：
  - 在 IQL / QMIX 智能体中记录最近 N 步动作索引。
  - 在 `_collect_training_metrics` 或专门方法中，按算法类型聚合动作频次。
- 可视化实现：
  - 新增 `ActionDistributionPanel`，按算法类型展示动作频率条形图或分组柱状图。

#### 5.2 奖励趋势与策略熵
- 奖励趋势：
  - 在训练统计中增加 `avg_reward` / `recent_reward`。
  - 新建 `Reward Trend` MultiLineChart，展示 IQL / QMIX 平均奖励随时间变化。
- 策略熵：
  - 基于动作频次分布计算熵，作为策略确定性的度量。
  - 在 Behavior 视图中新增 `Policy Entropy` 曲线图。

**验收标准**:
- [ ] 行为策略可视化清晰易懂，能够区分不同算法的动作偏好与策略形态
- [ ] 奖励与策略熵曲线能辅助判断训练是否收敛/稳定

---

## 四、技术细节

### 4.1 数据收集策略

```python
def _collect_training_metrics(self) -> Dict[str, TrainingMetrics]:
    """收集训练指标"""
    metrics_by_type = {}
    
    # 从智能体收集
    for agent in self.agent_manager.agents:
        if hasattr(agent, 'get_training_info'):
            agent_type = agent.agent_type.value
            training_info = agent.get_training_info()
            
            if agent_type not in metrics_by_type:
                metrics_by_type[agent_type] = TrainingMetrics(agent_type=agent_type)
            
            # 聚合数据
            metrics_by_type[agent_type].avg_loss += training_info.get('avg_loss', 0)
            # ... 其他指标
    
    # 从训练器收集（QMIX）
    for agent_type, trainer in self.marl_trainers.items():
        if hasattr(trainer, 'get_training_stats'):
            stats = trainer.get_training_stats()
            # 聚合训练器数据
    
    return metrics_by_type
```

### 4.2 图表更新策略

- **更新频率**: 每5步更新一次（可配置）
- **数据窗口**: 保留最近200个数据点
- **采样策略**: 如果数据点过多，进行均匀采样

### 4.3 布局设计

```
┌─────────────────────────────────────────┐
│  环境可视化区域                          │
│  (现有)                                  │
└─────────────────────────────────────────┘
┌─────────────────────────────────────────┐
│  控制面板 (现有)                         │
│  ┌───────────────────────────────────┐  │
│  │ 损失函数曲线                      │  │
│  │ [IQL] [QMIX]                      │  │
│  └───────────────────────────────────┘  │
│  ┌───────────────────────────────────┐  │
│  │ Q值趋势                           │  │
│  │ [IQL] [QMIX]                      │  │
│  └───────────────────────────────────┘  │
│  ┌───────────────────────────────────┐  │
│  │ TD误差 (阶段3)                    │  │
│  └───────────────────────────────────┘  │
│  ┌───────────────────────────────────┐  │
│  │ 探索率 (阶段3)                     │  │
│  └───────────────────────────────────┘  │
└─────────────────────────────────────────┘
```

## 五、风险评估

### 5.1 性能风险
- **风险**: 实时绘制多条曲线可能影响性能
- **缓解**: 使用数据采样和限制历史数据点数量

### 5.2 数据质量风险
- **风险**: 训练数据可能包含异常值
- **缓解**: 添加数据验证和异常值处理

### 5.3 兼容性风险
- **风险**: 新功能可能影响现有可视化
- **缓解**: 保持向后兼容，新功能可选启用

## 六、测试计划

### 6.1 单元测试
- 测试数据收集函数
- 测试图表更新逻辑
- 测试数据聚合算法

### 6.2 集成测试
- 测试完整的数据流（智能体 → 模拟 → 可视化）
- 测试多类型智能体场景
- 测试长时间运行稳定性

### 6.3 性能测试
- 测试不同智能体数量下的性能
- 测试图表更新对FPS的影响
- 测试内存使用情况

## 七、预期效果

### 7.1 短期效果（阶段1-2）
- ✅ 能够实时查看损失函数曲线
- ✅ 能够实时查看Q值变化趋势
- ✅ 了解模型收敛情况

### 7.2 中期效果（阶段3-4）
- ✅ 全面了解训练过程
- ✅ 能够诊断训练问题
- ✅ 优化训练参数

### 7.3 长期效果（阶段5）
- ✅ 深入理解智能体行为
- ✅ 改进算法设计
- ✅ 提升研究效率

## 八、时间估算

| 阶段 | 预计时间 | 累计时间 |
|------|---------|---------|
| 阶段1：数据收集 | 2-3小时 | 2-3小时 |
| 阶段2：基础图表 | 3-4小时 | 5-7小时 |
| 阶段3：高级指标 | 2-3小时 | 7-10小时 |
| 阶段4：优化增强 | 2-3小时 | 9-13小时 |
| 阶段5：行为可视化 | 4-5小时 | 13-18小时 |

**总计**: 13-18小时（不含阶段5为9-13小时）

## 九、下一步行动（结合当前项目状态）

1. **优先实现阶段4：视图系统 + 布局抽象**
   - 在代码层面完成 Overview / Training 基础视图切换和图表槽位抽象。
2. **在 Training 视图上稳定训练曲线体验**
   - 调整图表高度、legend、颜色映射，优化 IQL / QMIX 对比效果。
3. **启动阶段5：行为策略可视化的 MVP**
   - 先实现「按算法聚合的动作分布面板」与「奖励趋势曲线」，在 Behavior 视图中展示。
4. **基于使用体验与性能反馈继续迭代**
   - 根据实际运行 FPS、研究需求和你的反馈，调整哪些图表默认开启，哪些通过视图/交互按需查看。


